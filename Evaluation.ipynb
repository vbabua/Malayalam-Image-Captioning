{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ5krkHjHKKt"
      },
      "source": [
        "# Image Captioning Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR0NAZWnLNP9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dKgvRAOH7kO"
      },
      "source": [
        "# Setting up Dependencies\n",
        "\n",
        "Install the required libraries and import necessary modules for the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp1i7UeQH52I"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge"
      ],
      "metadata": {
        "id": "EvZE5JwZUKf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZS1q-9PLzKd"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import VisionEncoderDecoderModel\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaaOiRjmLBVm"
      },
      "source": [
        "#  Loading the Test Dataset\n",
        "Load the test dataset from the specified directory using the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWxKsXqDLJDt"
      },
      "outputs": [],
      "source": [
        "root = \"/content/drive/MyDrive/MSC_PROJECT/Flickr30k/Formatted_Flickr30k/\"\n",
        "\n",
        "test_dataset = load_dataset('imagefolder', data_dir = root, split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3Y4sRML40l"
      },
      "source": [
        "# Defining the Image Captioning Dataset\n",
        "Define a custom dataset class, ImageCaptioningDataset, to process and return the images and their corresponding captions in a format suitable for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHaRob9sL2OG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, ViTFeatureExtractor\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, feature_extractor, tokenizer, max_target_length=128):\n",
        "        self.dataset = dataset\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        # prepare image\n",
        "        pixel_values = self.feature_extractor(images=item[\"image\"], return_tensors=\"np\").pixel_values\n",
        "\n",
        "        # add captions by encoding the input\n",
        "        captions = self.tokenizer(\n",
        "            text=item[\"text\"], padding=\"max_length\", max_length=self.max_length\n",
        "        ).input_ids\n",
        "\n",
        "        encoding = {\n",
        "            \"pixel_values\": torch.from_numpy(pixel_values.squeeze()),\n",
        "            \"labels\": torch.tensor(captions),\n",
        "        }\n",
        "        return encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucQvO14PL_6_"
      },
      "source": [
        "# Preparing Tokenizer and Feature Extractor\n",
        "Load the necessary tokenizers and feature extractors for the vision and text models to be used. These are crucial for processing the input data correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NyHofOZMH6C"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "\n",
        "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "text_decoder_model = \"bipin/malayalam-gpt2\"\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(text_decoder_model)\n",
        "\"\"\"\n",
        "\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "\n",
        "\n",
        "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "text_decoder_model = \"l3cube-pune/malayalam-bert\"\n",
        "\n",
        "# image feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
        "# text tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(text_decoder_model)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yIKAp6CMJa9"
      },
      "source": [
        "# Pre-processing the Test Data\n",
        "Use the ImageCaptioningDataset class to process the raw test data, making it ready for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS530m5SMNnJ"
      },
      "outputs": [],
      "source": [
        "test_data = ImageCaptioningDataset(test_dataset, feature_extractor, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dUJDtMnB3FZ"
      },
      "outputs": [],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88RzcFOfICnR"
      },
      "source": [
        "# Loading the Pre-trained Model\n",
        "Load the pre-trained VisionEncoderDecoderModel from the specified path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff47r_9GIE5F"
      },
      "outputs": [],
      "source": [
        "\n",
        "#vit+ gpt2\n",
        "path = \"/content/drive/MyDrive/Flickr30k_Models/vitgptlr1e-4epochs10\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(path)\n",
        "\"\"\"\n",
        "\n",
        "path = \"/content/drive/MyDrive/Flickr30k_Models/bert/lre-6epoch20/vitbertlr1e-6epochs7.96\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(path)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS-HfzC2IJu9"
      },
      "source": [
        "# Generating Predictions\n",
        "Function to generate caption predictions for each image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTWO3e0QIN81"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_predictions(model, dataset, tokenizer):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    predictions = []\n",
        "    actual = []\n",
        "\n",
        "    with torch.no_grad():  # Do not calculate gradients\n",
        "        for batch in tqdm(dataset):  # Use tqdm for a progress bar\n",
        "            # Forward pass through the model\n",
        "            outputs = model.generate(pixel_values=batch[\"pixel_values\"].unsqueeze(0),\n",
        "                                     max_length=128,\n",
        "                                     do_sample=True)\n",
        "\n",
        "            # Decode the outputs and append to lists\n",
        "            predictions.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "            actual.append(tokenizer.decode(batch[\"labels\"], skip_special_tokens=True))\n",
        "\n",
        "    return predictions, actual\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnA-Wjd8IWHY"
      },
      "outputs": [],
      "source": [
        "predictions, actual = generate_predictions(model, test_data, tokenizer )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBvtvn_UIWsM"
      },
      "source": [
        "# Embedding the Captions\n",
        "Embed the generated predictions and the actual captions into vector representations using the sentence-transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg1uAeT7Io2t"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#model = SentenceTransformer('bipin/malayalam-gpt2')\n",
        "transformer = SentenceTransformer('l3cube-pune/malayalam-bert')\n",
        "malayalam_embeddings = transformer.encode(predictions)\n",
        "english_embeddings = transformer.encode(actual)\n",
        "malayalam_embeddings = torch.tensor(malayalam_embeddings)\n",
        "english_embeddings = torch.tensor(english_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpRi6szIrgD"
      },
      "source": [
        "# Cosine Similarity Calculation\n",
        "\n",
        "Compute the cosine similarity between the embeddings of the predictions and the actual captions. This measures the similarity between the generated and actual captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcreNm7gIqeN"
      },
      "outputs": [],
      "source": [
        "similarities = cosine_similarity(malayalam_embeddings, english_embeddings)\n",
        "mean_similarity = np.mean(np.diag(similarities))\n",
        "print('Mean Cosine Similarity:', mean_similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gKKSBvIx9Q"
      },
      "source": [
        "# Metrics Computation for Predictions\n",
        "Generate predictions in a format suitable for metric computation, and define a function to compute various evaluation metrics such as BLEU, METEOR, and ROUGE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ARdZZXRI7Sz"
      },
      "outputs": [],
      "source": [
        "def generate_predictions_for_metrics(model, dataset):\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():  # Do not calculate gradients\n",
        "        for batch in dataset:  # Iterate over batches in the dataset\n",
        "            # Forward pass through the model\n",
        "            outputs = model.generate(pixel_values=batch[\"pixel_values\"].unsqueeze(0),\n",
        "                                     max_length=128,\n",
        "                                     do_sample=True)\n",
        "\n",
        "            # Append to lists\n",
        "            predictions.append(outputs[0].cpu().numpy())\n",
        "            labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    return predictions, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akSQpZ5jI8Ag"
      },
      "outputs": [],
      "source": [
        "test_predictions, test_labels = generate_predictions_for_metrics(model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm6LcCYbI972"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ci4LoKgJANt"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = [rouge.get_scores(pred, ref[0]) for pred, ref in zip(decoded_preds, decoded_labels)]\n",
        "    rouge_1 = np.mean([score[0]['rouge-1']['f'] for score in rouge_scores])\n",
        "    rouge_2 = np.mean([score[0]['rouge-2']['f'] for score in rouge_scores])\n",
        "    rouge_l = np.mean([score[0]['rouge-l']['f'] for score in rouge_scores])\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    tokenized_preds = [tokenizer.tokenize(pred) for pred in decoded_preds]\n",
        "    tokenized_labels = [[tokenizer.tokenize(label)] for label in decoded_labels]\n",
        "    smoothing = SmoothingFunction().method7\n",
        "    bleu1 = corpus_bleu(tokenized_labels, tokenized_preds, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
        "    bleu2 = corpus_bleu(tokenized_labels, tokenized_preds, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
        "    bleu3 = corpus_bleu(tokenized_labels, tokenized_preds, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
        "    bleu4 = corpus_bleu(tokenized_labels, tokenized_preds, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
        "\n",
        "    # Compute METEOR scores\n",
        "    meteor_scores = [meteor_score([ref.split()], pred.split()) for ref, pred in zip(decoded_labels, decoded_preds)]\n",
        "    mean_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "    # Compute individual BLEU-4 scores for each pair of prediction and label, include the index in the tuple\n",
        "    individual_bleu4_scores = [\n",
        "        (index, corpus_bleu([label_tokens], [pred_tokens], weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing), pred, label)\n",
        "        for index, (pred_tokens, label_tokens, pred, label) in enumerate(zip(tokenized_preds, tokenized_labels, decoded_preds, decoded_labels))\n",
        "    ]\n",
        "    # Sort the list of tuples by BLEU-4 score in descending order\n",
        "    individual_bleu4_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the BLEU, METEOR, and ROUGE scores in a format compatible with Trainer's logging\n",
        "    return {\n",
        "        \"bleu1\": bleu1,\n",
        "        \"bleu2\": bleu2,\n",
        "        \"bleu3\": bleu3,\n",
        "        \"bleu4\": bleu4,\n",
        "        \"meteor\": mean_meteor,\n",
        "        \"rouge-1\": rouge_1,\n",
        "        \"rouge-2\": rouge_2,\n",
        "        \"rouge-l\": rouge_l,\n",
        "        \"highest_bleu4\": individual_bleu4_scores  # The pair with the highest BLEU-4 score\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3Dy4dlAJdTZ"
      },
      "outputs": [],
      "source": [
        "test_metrics = compute_metrics((test_predictions, test_labels))\n",
        "print(test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKUwMAkLJiRG"
      },
      "source": [
        "# Visualization and Analysis\n",
        "Visualize an example from the test dataset and its corresponding prediction. This provides a qualitative understanding of how well the model is performing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNUwBO8kJmcY"
      },
      "outputs": [],
      "source": [
        "processed_example = test_data[939]\n",
        "image_tensor = processed_example[\"pixel_values\"]\n",
        "plt.imshow(image_tensor.permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "caption = tokenizer.decode(processed_example[\"labels\"], skip_special_tokens=True)\n",
        "print(\"Caption:\", caption)\n",
        "text = tokenizer.decode(test_predictions[939], skip_special_tokens=True)\n",
        "print(\"Predictions:\", text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPPbPV19OaX1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}